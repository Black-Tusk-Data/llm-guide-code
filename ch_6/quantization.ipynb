{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cac2dc-d3d0-4607-9edf-7e65e11e7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b79e3f-2cf9-469d-a920-16b54ccecf75",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Resources:\n",
    " - https://arxiv.org/abs/2208.07339"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827bbb3-680d-49b8-9a77-18e011a3ef54",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Given what we know about a transformer-based language model, how can we estimate how much memory a given model needs?\n",
    "\n",
    "Let's investigate the size of the model a little more.\n",
    "\n",
    "Since Phi3-mini-4k is advertised as a 3.8B parameter model, we're expecting 3.8 billion parameters.\n",
    "\n",
    "Let's verify that quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ccf670d-69c7-4146-9d07-fc73417a78a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6d27da-1bdd-4dd4-88cb-84fe7eb48746",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading shards:   0%|                                                                                                                                                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading shards:  50%|█████████████████████████████████████████████████████████████████████████████▌                                                                             | 1/2 [00:09<00:09,  9.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████                                                                          | 1/2 [00:10<00:10, 10.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3722578944"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_id, device_map=\"cuda\")\n",
    "sum(p.numel() for p in model.parameters()) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af3a3be-f861-4799-8e5e-6cdb56267e25",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.722578944"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3722578944 / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa0c49-5999-47f6-9e82-db3f19ce6e26",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "So confirmed, we have about 3.7 billion parameters to load into the GPU.\n",
    "Remember, each parameter is just a number!\n",
    "\n",
    "By default, the data type used to represent each parameter / number is a 32-bit float, which gives us\n",
    "\n",
    " (32 / 8) * 3.72e9 billion parameters / 1e9 bytes per GB = 14.88 GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf74c7c7-4a9e-4e0b-bac9-2825f70ecebb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.88"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(32 / 8) * 3.72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9093ad-414a-4a70-925c-9cde2364c962",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Thus in theory, we'll need at least 14.88 GB of GPU RAM just to load the `microsoft/Phi-3-mini-4k-instruct` model into the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7aab37-d824-4a55-8ddf-dc43b0f8791a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's take a look at how memory usage is affected in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49556c0e-22df-4fe9-ae0a-224859bf276c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e005ee-8cb4-4ef6-aabe-1b6647fc02c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def check_model_size_on_gpu_mb(quantization_config: BitsAndBytesConfig) -> float:\n",
    "    size_before = torch.cuda.memory_allocated()\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "    model_size = torch.cuda.memory_allocated() - size_before\n",
    "    del model\n",
    "    return model_size / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b1c35-9774-43f4-bec9-b0e2c2f1dbea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Note that I'm restarting the jupyter kernel between each invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e966798-5322-4f38-b24a-2f37fcf0085e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|█████████████████████████████████                                 | 1/2 [00:05<00:05,  5.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full size: 14890.594304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Full size:\", check_model_size_on_gpu_mb(quantization_config=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5afabebb-cffa-4cc1-aeb2-c679005e7c08",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|█████████████████████████████████                                 | 1/2 [00:03<00:03,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit size: 3859.044352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"8-bit size:\", check_model_size_on_gpu_mb(quantization_config=BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac81473-7ba8-4ec1-9f58-15e86db4e6ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|█████████████████████████████████                                 | 1/2 [00:03<00:03,  3.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit size: 2240.436224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"4-bit size:\", check_model_size_on_gpu_mb(quantization_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0829b-7bbb-414d-aa0f-3b19e75c8201",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Note that the memory usage does not perfectly linearly decrease as we quantize, mainly because we aren't quantizing every single parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20acfdb0-9abf-4210-a2e9-b7bad591d349",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "All the BitsAndBytesConfig options:\n",
    "\n",
    "        load_in_8bit (`bool`, *optional*, defaults to `False`):\n",
    "            This flag is used to enable 8-bit quantization with LLM.int8().\n",
    "        load_in_4bit (`bool`, *optional*, defaults to `False`):\n",
    "            This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from\n",
    "            `bitsandbytes`.\n",
    "        llm_int8_threshold (`float`, *optional*, defaults to 6.0):\n",
    "            This corresponds to the outlier threshold for outlier detection as described in `LLM.int8() : 8-bit Matrix\n",
    "            Multiplication for Transformers at Scale` paper: https://arxiv.org/abs/2208.07339 Any hidden states value\n",
    "            that is above this threshold will be considered an outlier and the operation on those values will be done\n",
    "            in fp16. Values are usually normally distributed, that is, most values are in the range [-3.5, 3.5], but\n",
    "            there are some exceptional systematic outliers that are very differently distributed for large models.\n",
    "            These outliers are often in the interval [-60, -6] or [6, 60]. Int8 quantization works well for values of\n",
    "            magnitude ~5, but beyond that, there is a significant performance penalty. A good default threshold is 6,\n",
    "            but a lower threshold might be needed for more unstable models (small models, fine-tuning).\n",
    "        llm_int8_skip_modules (`List[str]`, *optional*):\n",
    "            An explicit list of the modules that we do not want to convert in 8-bit. This is useful for models such as\n",
    "            Jukebox that has several heads in different places and not necessarily at the last position. For example\n",
    "            for `CausalLM` models, the last `lm_head` is kept in its original `dtype`.\n",
    "        llm_int8_enable_fp32_cpu_offload (`bool`, *optional*, defaults to `False`):\n",
    "            This flag is used for advanced use cases and users that are aware of this feature. If you want to split\n",
    "            your model in different parts and run some parts in int8 on GPU and some parts in fp32 on CPU, you can use\n",
    "            this flag. This is useful for offloading large models such as `google/flan-t5-xxl`. Note that the int8\n",
    "            operations will not be run on CPU.\n",
    "        llm_int8_has_fp16_weight (`bool`, *optional*, defaults to `False`):\n",
    "            This flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning as the weights do not\n",
    "            have to be converted back and forth for the backward pass.\n",
    "        bnb_4bit_compute_dtype (`torch.dtype` or str, *optional*, defaults to `torch.float32`):\n",
    "            This sets the computational type which might be different than the input type. For example, inputs might be\n",
    "            fp32, but computation can be set to bf16 for speedups.\n",
    "        bnb_4bit_quant_type (`str`,  *optional*, defaults to `\"fp4\"`):\n",
    "            This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data types\n",
    "            which are specified by `fp4` or `nf4`.\n",
    "        bnb_4bit_use_double_quant (`bool`, *optional*, defaults to `False`):\n",
    "            This flag is used for nested quantization where the quantization constants from the first quantization are\n",
    "            quantized again.\n",
    "        bnb_4bit_quant_storage (`torch.dtype` or str, *optional*, defaults to `torch.uint8`):\n",
    "            This sets the storage type to pack the quanitzed 4-bit prarams."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "name": "quantization.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
