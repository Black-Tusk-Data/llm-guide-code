{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d338aa-40ca-4517-9c7f-e080f144e69e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:   3% (60/1974)\u001b[K\r",
      "remote: Compressing objects:   4% (79/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:   5% (99/1974)\u001b[K\r",
      "remote: Compressing objects:   6% (119/1974)\u001b[K\r",
      "remote: Compressing objects:   7% (139/1974)\u001b[K\r",
      "remote: Compressing objects:   8% (158/1974)\u001b[K\r",
      "remote: Compressing objects:   9% (178/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  10% (198/1974)\u001b[K\r",
      "remote: Compressing objects:  11% (218/1974)\u001b[K\r",
      "remote: Compressing objects:  12% (237/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  13% (257/1974)\u001b[K\r",
      "remote: Compressing objects:  14% (277/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  15% (297/1974)\u001b[K\r",
      "remote: Compressing objects:  16% (316/1974)\u001b[K\r",
      "remote: Compressing objects:  17% (336/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  18% (356/1974)\u001b[K\r",
      "remote: Compressing objects:  19% (376/1974)\u001b[K\r",
      "remote: Compressing objects:  20% (395/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  21% (415/1974)\u001b[K\r",
      "remote: Compressing objects:  22% (435/1974)\u001b[K\r",
      "remote: Compressing objects:  23% (455/1974)\u001b[K\r",
      "remote: Compressing objects:  24% (474/1974)\u001b[K\r",
      "remote: Compressing objects:  25% (494/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  26% (514/1974)\u001b[K\r",
      "remote: Compressing objects:  27% (533/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  28% (553/1974)\u001b[K\r",
      "remote: Compressing objects:  29% (573/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  30% (593/1974)\u001b[K\r",
      "remote: Compressing objects:  31% (612/1974)\u001b[K\r",
      "remote: Compressing objects:  32% (632/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  33% (652/1974)\u001b[K\r",
      "remote: Compressing objects:  34% (672/1974)\u001b[K\r",
      "remote: Compressing objects:  35% (691/1974)\u001b[K\r",
      "remote: Compressing objects:  36% (711/1974)\u001b[K\r",
      "remote: Compressing objects:  37% (731/1974)\u001b[K\r",
      "remote: Compressing objects:  38% (751/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  39% (770/1974)\u001b[K\r",
      "remote: Compressing objects:  40% (790/1974)\u001b[K\r",
      "remote: Compressing objects:  41% (810/1974)\u001b[K\r",
      "remote: Compressing objects:  42% (830/1974)\u001b[K\r",
      "remote: Compressing objects:  43% (849/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  44% (869/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  44% (884/1974)\u001b[K\r",
      "remote: Compressing objects:  45% (889/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  46% (909/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  47% (928/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  48% (948/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  49% (968/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  50% (987/1974)\u001b[K\r",
      "remote: Compressing objects:  51% (1007/1974)\u001b[K\r",
      "remote: Compressing objects:  52% (1027/1974)\u001b[K\r",
      "remote: Compressing objects:  53% (1047/1974)\u001b[K\r",
      "remote: Compressing objects:  54% (1066/1974)\u001b[K\r",
      "remote: Compressing objects:  55% (1086/1974)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  56% (1106/1974)\u001b[K\r",
      "remote: Compressing objects:  57% (1126/1974)\u001b[K\r",
      "remote: Compressing objects:  58% (1145/1974)\u001b[K\r",
      "remote: Compressing objects:  59% (1165/1974)\u001b[K\r",
      "remote: Compressing objects:  60% (1185/1974)\u001b[K\r",
      "remote: Compressing objects:  61% (1205/1974)\u001b[K\r",
      "remote: Compressing objects:  62% (1224/1974)\u001b[K\r",
      "remote: Compressing objects:  63% (1244/1974)\u001b[K\r",
      "remote: Compressing objects:  64% (1264/1974)\u001b[K\r",
      "remote: Compressing objects:  65% (1284/1974)\u001b[K\r",
      "remote: Compressing objects:  66% (1303/1974)\u001b[K\r",
      "remote: Compressing objects:  67% (1323/1974)\u001b[K\r",
      "remote: Compressing objects:  68% (1343/1974)\u001b[K\r",
      "remote: Compressing objects:  69% (1363/1974)\u001b[K\r",
      "remote: Compressing objects:  70% (1382/1974)\u001b[K\r",
      "remote: Compressing objects:  71% (1402/1974)\u001b[K\r",
      "remote: Compressing objects:  72% (1422/1974)\u001b[K\r",
      "remote: Compressing objects:  73% (1442/1974)\u001b[K\r",
      "remote: Compressing objects:  74% (1461/1974)\u001b[K\r",
      "remote: Compressing objects:  75% (1481/1974)\u001b[K\r",
      "remote: Compressing objects:  76% (1501/1974)\u001b[K\r",
      "remote: Compressing objects:  77% (1520/1974)\u001b[K\r",
      "remote: Compressing objects:  78% (1540/1974)\u001b[K\r",
      "remote: Compressing objects:  79% (1560/1974)\u001b[K\r",
      "remote: Compressing objects:  80% (1580/1974)\u001b[K\r",
      "remote: Compressing objects:  81% (1599/1974)\u001b[K\r",
      "remote: Compressing objects:  82% (1619/1974)\u001b[K\r",
      "remote: Compressing objects:  83% (1639/1974)\u001b[K\r",
      "remote: Compressing objects:  84% (1659/1974)\u001b[K\r",
      "remote: Compressing objects:  85% (1678/1974)\u001b[K\r",
      "remote: Compressing objects:  86% (1698/1974)\u001b[K\r",
      "remote: Compressing objects:  87% (1718/1974)\u001b[K\r",
      "remote: Compressing objects:  88% (1738/1974)\u001b[K\r",
      "remote: Compressing objects:  89% (1757/1974)\u001b[K\r",
      "remote: Compressing objects:  90% (1777/1974)\u001b[K\r",
      "remote: Compressing objects:  91% (1797/1974)\u001b[K\r",
      "remote: Compressing objects:  92% (1817/1974)\u001b[K\r",
      "remote: Compressing objects:  93% (1836/1974)\u001b[K\r",
      "remote: Compressing objects:  94% (1856/1974)\u001b[K\r",
      "remote: Compressing objects:  95% (1876/1974)\u001b[K\r",
      "remote: Compressing objects:  96% (1896/1974)\u001b[K\r",
      "remote: Compressing objects:  97% (1915/1974)\u001b[K\r",
      "remote: Compressing objects:  98% (1935/1974)\u001b[K\r",
      "remote: Compressing objects:  99% (1955/1974)\u001b[K\r",
      "remote: Compressing objects: 100% (1974/1974)\u001b[K\r",
      "remote: Compressing objects: 100% (1974/1974), done.\u001b[K\r\n",
      "Receiving objects:   0% (1/233244)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   1% (2333/233244)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   2% (4665/233244), 2.25 MiB | 3.99 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   2% (6941/233244), 2.25 MiB | 3.99 MiB/s\r",
      "Receiving objects:   3% (6998/233244), 2.25 MiB | 3.99 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   4% (9330/233244), 4.09 MiB | 3.74 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   5% (11663/233244), 5.91 MiB | 3.71 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   5% (12990/233244), 5.91 MiB | 3.71 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   6% (13995/233244), 7.84 MiB | 3.72 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   7% (16328/233244), 7.84 MiB | 3.72 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   8% (18660/233244), 9.68 MiB | 3.64 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   8% (18871/233244), 9.68 MiB | 3.64 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   9% (20992/233244), 10.80 MiB | 3.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   9% (22217/233244), 11.55 MiB | 3.13 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:   9% (22834/233244), 13.08 MiB | 2.79 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  10% (23325/233244), 14.80 MiB | 2.32 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  10% (23490/233244), 14.80 MiB | 2.32 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  10% (23928/233244), 16.61 MiB | 1.90 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  10% (24816/233244), 18.61 MiB | 1.70 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  11% (25657/233244), 20.78 MiB | 1.84 MiB/s\r",
      "Receiving objects:  11% (25708/233244), 20.78 MiB | 1.84 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  11% (26672/233244), 23.14 MiB | 2.00 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  11% (27190/233244), 25.64 MiB | 2.15 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  11% (27956/233244), 27.79 MiB | 2.22 MiB/s\r",
      "Receiving objects:  12% (27990/233244), 27.79 MiB | 2.22 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  12% (29226/233244), 30.07 MiB | 2.26 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  12% (30277/233244), 32.59 MiB | 2.32 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  13% (30322/233244), 32.59 MiB | 2.32 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  13% (31350/233244), 35.11 MiB | 2.34 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  14% (32655/233244), 36.47 MiB | 2.36 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  14% (32859/233244), 37.79 MiB | 2.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  14% (34554/233244), 40.41 MiB | 2.52 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  15% (34987/233244), 40.41 MiB | 2.52 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  15% (36352/233244), 43.16 MiB | 2.59 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  16% (37320/233244), 43.16 MiB | 2.59 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  16% (38636/233244), 46.07 MiB | 2.67 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  17% (39652/233244), 47.60 MiB | 2.74 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  17% (40481/233244), 49.08 MiB | 2.77 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  18% (41984/233244), 50.75 MiB | 2.84 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  18% (43142/233244), 50.75 MiB | 2.84 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  19% (44317/233244), 52.61 MiB | 2.96 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  20% (46649/233244), 52.61 MiB | 2.96 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  21% (48982/233244), 54.56 MiB | 3.10 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  21% (49528/233244), 54.56 MiB | 3.10 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  22% (51314/233244), 56.62 MiB | 3.25 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  22% (53194/233244), 59.06 MiB | 3.48 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  23% (53647/233244), 61.69 MiB | 3.75 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  24% (55979/233244), 64.52 MiB | 4.05 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  25% (58311/233244), 64.52 MiB | 4.05 MiB/s\r",
      "Receiving objects:  25% (58348/233244), 64.52 MiB | 4.05 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  26% (60644/233244), 67.43 MiB | 4.35 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  26% (62540/233244), 69.22 MiB | 4.42 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  27% (62976/233244), 70.99 MiB | 4.45 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  28% (65309/233244), 72.78 MiB | 4.45 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  28% (66440/233244), 72.78 MiB | 4.45 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  29% (67641/233244), 74.77 MiB | 4.46 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  30% (69974/233244), 76.72 MiB | 4.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  30% (71151/233244), 76.72 MiB | 4.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  31% (72306/233244), 78.21 MiB | 4.19 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  31% (74272/233244), 79.79 MiB | 3.96 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  32% (74639/233244), 81.32 MiB | 3.68 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  32% (76461/233244), 82.95 MiB | 3.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  33% (76971/233244), 84.61 MiB | 3.37 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  33% (79097/233244), 86.28 MiB | 3.35 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  34% (79303/233244), 86.28 MiB | 3.35 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  34% (80079/233244), 89.73 MiB | 3.27 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  35% (81636/233244), 91.45 MiB | 3.24 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  36% (83968/233244), 93.20 MiB | 3.29 MiB/s\r",
      "Receiving objects:  36% (84086/233244), 93.20 MiB | 3.29 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  37% (86301/233244), 94.99 MiB | 3.33 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  37% (87121/233244), 96.85 MiB | 3.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  38% (88633/233244), 98.69 MiB | 3.45 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  38% (90402/233244), 100.66 MiB | 3.52 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  39% (90966/233244), 100.66 MiB | 3.52 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  39% (93090/233244), 104.54 MiB | 3.63 MiB/s\r",
      "Receiving objects:  40% (93298/233244), 104.54 MiB | 3.63 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  41% (95631/233244), 108.66 MiB | 3.78 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  41% (96416/233244), 108.66 MiB | 3.78 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  42% (97963/233244), 110.87 MiB | 3.89 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  43% (100295/233244), 113.25 MiB | 4.02 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  43% (100465/233244), 113.25 MiB | 4.02 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  44% (102628/233244), 115.78 MiB | 4.17 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  44% (104172/233244), 118.62 MiB | 4.39 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  45% (104960/233244), 121.15 MiB | 4.51 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  46% (107293/233244), 123.41 MiB | 4.58 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  46% (108134/233244), 123.41 MiB | 4.58 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  47% (109625/233244), 125.99 MiB | 4.73 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  48% (111958/233244), 128.61 MiB | 4.83 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  48% (113083/233244), 128.61 MiB | 4.83 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  49% (114290/233244), 128.61 MiB | 4.83 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  50% (116622/233244), 131.46 MiB | 5.00 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  51% (118955/233244), 134.21 MiB | 5.12 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  51% (119641/233244), 134.21 MiB | 5.12 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  52% (121287/233244), 134.21 MiB | 5.12 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  53% (123620/233244), 137.18 MiB | 5.25 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  54% (125952/233244), 139.67 MiB | 5.24 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  54% (126897/233244), 139.67 MiB | 5.24 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  55% (128285/233244), 139.67 MiB | 5.24 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  56% (130617/233244), 141.86 MiB | 5.11 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  57% (132950/233244), 141.86 MiB | 5.11 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  58% (135282/233244), 144.18 MiB | 5.08 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  58% (135888/233244), 144.18 MiB | 5.08 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  59% (137614/233244), 146.71 MiB | 5.14 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  60% (139947/233244), 146.71 MiB | 5.14 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  61% (142279/233244), 149.14 MiB | 5.11 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  61% (142893/233244), 149.14 MiB | 5.11 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  62% (144612/233244), 149.14 MiB | 5.11 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  63% (146944/233244), 151.76 MiB | 5.11 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  64% (149277/233244), 151.76 MiB | 5.11 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  65% (151609/233244), 154.38 MiB | 5.06 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  66% (153942/233244), 154.38 MiB | 5.06 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  66% (154428/233244), 154.38 MiB | 5.06 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  67% (156274/233244), 154.38 MiB | 5.06 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  68% (158606/233244), 156.91 MiB | 5.01 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  69% (160939/233244), 156.91 MiB | 5.01 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  70% (163271/233244), 159.51 MiB | 4.93 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  70% (164147/233244), 159.51 MiB | 4.93 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  71% (165604/233244), 159.51 MiB | 4.93 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  72% (167936/233244), 159.51 MiB | 4.93 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  73% (170269/233244), 162.10 MiB | 4.96 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  74% (172601/233244), 162.10 MiB | 4.96 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  75% (174933/233244), 162.10 MiB | 4.96 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  76% (177266/233244), 162.10 MiB | 4.96 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  77% (179598/233244), 164.75 MiB | 5.05 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  78% (181931/233244), 164.75 MiB | 5.05 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  78% (182878/233244), 164.75 MiB | 5.05 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  79% (184263/233244), 164.75 MiB | 5.05 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  80% (186596/233244), 164.75 MiB | 5.05 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  81% (188928/233244), 167.25 MiB | 5.09 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  81% (190508/233244), 169.91 MiB | 5.12 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  82% (191261/233244), 169.91 MiB | 5.12 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  83% (193593/233244), 175.21 MiB | 5.19 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  83% (193996/233244), 175.21 MiB | 5.19 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  84% (195925/233244), 177.80 MiB | 5.16 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  84% (196642/233244), 180.59 MiB | 5.22 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  85% (198258/233244), 183.29 MiB | 5.25 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  85% (200405/233244), 186.27 MiB | 5.33 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  86% (200590/233244), 186.27 MiB | 5.33 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  87% (202923/233244), 189.21 MiB | 5.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  87% (203662/233244), 192.31 MiB | 5.54 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  88% (205255/233244), 195.62 MiB | 5.68 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  88% (205854/233244), 199.10 MiB | 5.87 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  89% (207588/233244), 206.62 MiB | 6.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  89% (208978/233244), 206.62 MiB | 6.40 MiB/s\r",
      "Receiving objects:  90% (209920/233244), 206.62 MiB | 6.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  91% (212253/233244), 206.62 MiB | 6.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  92% (214585/233244), 206.62 MiB | 6.40 MiB/s\r",
      "Receiving objects:  93% (216917/233244), 206.62 MiB | 6.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  94% (219250/233244), 206.62 MiB | 6.40 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  95% (221582/233244), 209.95 MiB | 6.50 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  95% (222271/233244), 213.16 MiB | 6.61 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  96% (223915/233244), 213.16 MiB | 6.61 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  97% (226247/233244), 220.32 MiB | 6.87 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  97% (226491/233244), 220.32 MiB | 6.87 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  98% (228580/233244), 224.07 MiB | 7.00 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  98% (229333/233244), 227.96 MiB | 7.13 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  99% (230912/233244), 231.90 MiB | 7.23 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  99% (232404/233244), 236.09 MiB | 7.36 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Total 233244 (delta 24415), reused 23349 (delta 23113), pack-reused 208097 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (233244/233244), 236.09 MiB | 7.36 MiB/s\r",
      "Receiving objects: 100% (233244/233244), 239.05 MiB | 4.10 MiB/s, done.\r\n",
      "Resolving deltas:   0% (0/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:   1% (1708/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:   2% (3416/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:   3% (5123/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:   4% (6831/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:   5% (8538/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:   6% (10246/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:   7% (11953/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:   8% (13661/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:   9% (15368/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  10% (17076/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  11% (18783/170753)\r",
      "Resolving deltas:  11% (18868/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  12% (20491/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  13% (22198/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  14% (23906/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  15% (25613/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  16% (27321/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  17% (29029/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  18% (30736/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  19% (32444/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  20% (34151/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  21% (35859/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  22% (37566/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  23% (39274/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  23% (40235/170753)\r",
      "Resolving deltas:  24% (40981/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  25% (42689/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  26% (44396/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  27% (46105/170753)\r",
      "Resolving deltas:  28% (47811/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  29% (49519/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  30% (51226/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  31% (52934/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  32% (54641/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  33% (56349/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  34% (58057/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  35% (59764/170753)\r",
      "Resolving deltas:  36% (61473/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  37% (63179/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  38% (64887/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  38% (66244/170753)\r",
      "Resolving deltas:  39% (66595/170753)\r",
      "Resolving deltas:  40% (68302/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  41% (70009/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  42% (71718/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  43% (73424/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  44% (75132/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  45% (76840/170753)\r",
      "Resolving deltas:  46% (78547/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  47% (80254/170753)\r",
      "Resolving deltas:  48% (81962/170753)\r",
      "Resolving deltas:  49% (83669/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  50% (85377/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  51% (87085/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  52% (88792/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  53% (90500/170753)\r",
      "Resolving deltas:  54% (92207/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  55% (93915/170753)\r",
      "Resolving deltas:  56% (95622/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  57% (97330/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  58% (99037/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  59% (100745/170753)\r",
      "Resolving deltas:  60% (102452/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  61% (104160/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  61% (105693/170753)\r",
      "Resolving deltas:  62% (105867/170753)\r",
      "Resolving deltas:  63% (107575/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  64% (109282/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  65% (110990/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  66% (112697/170753)\r",
      "Resolving deltas:  67% (114405/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  68% (116113/170753)\r",
      "Resolving deltas:  69% (117820/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  70% (119529/170753)\r",
      "Resolving deltas:  71% (121235/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  72% (122943/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  73% (124650/170753)\r",
      "Resolving deltas:  74% (126358/170753)\r",
      "Resolving deltas:  75% (128065/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  76% (129773/170753)\r",
      "Resolving deltas:  77% (131481/170753)\r",
      "Resolving deltas:  78% (133188/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  79% (134895/170753)\r",
      "Resolving deltas:  80% (136603/170753)\r",
      "Resolving deltas:  81% (138310/170753)\r",
      "Resolving deltas:  82% (140019/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  83% (141726/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  84% (143433/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  85% (145141/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  86% (146848/170753)\r",
      "Resolving deltas:  87% (148556/170753)\r",
      "Resolving deltas:  88% (150263/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  89% (151971/170753)\r",
      "Resolving deltas:  90% (153678/170753)\r",
      "Resolving deltas:  91% (155386/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  92% (157093/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  93% (158801/170753)\r",
      "Resolving deltas:  93% (158841/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  94% (160508/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  95% (162216/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  96% (163924/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  97% (165631/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  98% (167338/170753)\r",
      "Resolving deltas:  98% (167444/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas:  99% (169046/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas: 100% (170753/170753)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving deltas: 100% (170753/170753), done.\r\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfcbe36-ead2-4252-83a3-c9663b05bbf7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_file(path: Path) -> str:\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read()\n",
    "    pass\n",
    "\n",
    "root = Path(\"./transformers/docs/source/en\")\n",
    "df = pd.DataFrame.from_records([{\n",
    "    \"path\": str(path),\n",
    "    \"body\": read_file(path),\n",
    "} for path in root.rglob(\"*.md\") if path.is_file()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25172cf-eb7f-48e2-a8de-c75ebfc5b6e6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def word_length(text: str) -> int:\n",
    "    return len(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854eac17-eed8-4072-aaca-a062a02128e6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df[\"word_length\"] = df[\"body\"].apply(word_length)\n",
    "df = df.set_index(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263c8b06-070c-47e9-89f4-48ad277018da",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(\"transformers/docs/source/en/index.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2751338-3001-4775-ae28-80df5c09db35",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def md_header_split(\n",
    "        body: str,\n",
    "        depth: int = 1\n",
    "):\n",
    "    exp = re.compile(f\"^{'#' * depth} .*$\")\n",
    "    linenums: list[int] = []\n",
    "    lines = body.split(\"\\n\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if exp.search(line):\n",
    "            linenums.append(i)\n",
    "            pass\n",
    "        pass\n",
    "\n",
    "    # each linenum is the start of a new section\n",
    "    return [\n",
    "        \"\\n\".join(lines[i:j])\n",
    "        for i, j in zip(\n",
    "                [0] + linenums,\n",
    "                linenums + [len(lines) + 1],\n",
    "        )\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff605838-8252-4621-8687-3fee10bb0a66",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "THRESHOLD_WORDS = 600\n",
    "THRESHOLD_DEPTH = 4             # give up trying to split on headers at this point\n",
    "\n",
    "\n",
    "def naive_linebreak_split(text: str) -> list[str]:\n",
    "    # we just split and group on newlines\n",
    "    pieces = text.split(\"\\n\\n\")\n",
    "    return_pieces = [pieces[0]]\n",
    "    for piece in pieces[1:]:\n",
    "        if word_length(return_pieces[-1]) + word_length(piece) <= THRESHOLD_WORDS:\n",
    "            return_pieces[-1] = \"\\n\\n\".join([\n",
    "                return_pieces[-1],\n",
    "                piece,\n",
    "            ])\n",
    "            pass\n",
    "        else:\n",
    "            return_pieces.append(piece)\n",
    "        pass\n",
    "\n",
    "    return return_pieces\n",
    "\n",
    "def recursive_header_split(\n",
    "        body: str,\n",
    "        depth: int = 1,\n",
    ") -> list[str]:\n",
    "    if word_length(body) <= THRESHOLD_WORDS:\n",
    "        return [body]\n",
    "\n",
    "    if depth == THRESHOLD_DEPTH:\n",
    "        return naive_linebreak_split(body)\n",
    "\n",
    "    header_splits = md_header_split(body, depth)\n",
    "    return_splits = []\n",
    "    for piece in header_splits:\n",
    "        if word_length(piece) < THRESHOLD_WORDS:\n",
    "            return_splits.append(piece)\n",
    "        else:\n",
    "            return_splits.extend(\n",
    "                recursive_header_split(piece, depth=depth+1)\n",
    "            )\n",
    "        pass\n",
    "    return return_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9bf866-d581-4d4f-93c8-399092835fd7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df[\"chunks\"]= df[\"body\"].apply(recursive_header_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3eabe49-c893-4844-87ae-cfc40aea24b6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "chunk_text_by_id = {\n",
    "    f\"{path}:{i}\": chunk\n",
    "     for path, row in df[[\"chunks\"]].iterrows()\n",
    "    for i, chunk in enumerate(row[\"chunks\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18e71d11-3220-4a10-a22b-70682fa26ac2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "device = \"cuda\"\n",
    "embedding_model_id = \"jinaai/jina-embeddings-v2-base-en\"\n",
    "embedding_model = AutoModel.from_pretrained(\n",
    "    embedding_model_id,\n",
    "    trust_remote_code=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a7b696f-df8e-4b10-8728-a83c3bd644c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2883/2883 [01:10<00:00, 39.59it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 1\n",
    "embeddings = {}\n",
    "chunk_ids = list(chunk_text_by_id.keys())\n",
    "chunk_texts = list(chunk_text_by_id.values())\n",
    "\n",
    "progress_bar = tqdm(range(len(chunk_ids)))\n",
    "for i in range(0, len(chunk_ids), batch_size):\n",
    "    batch_embeddings = embedding_model.encode(\n",
    "        chunk_texts[i:i+batch_size],\n",
    "        device=device,\n",
    "    )\n",
    "    for chunk_id, embedding in zip(\n",
    "            chunk_ids[i:i+batch_size],\n",
    "            batch_embeddings\n",
    "    ):\n",
    "        embeddings[chunk_id] = embedding\n",
    "        pass\n",
    "    progress_bar.update(batch_size)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8d52d5a-ffe2-4df0-9b81-08c6c2421737",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a5f411a-cc08-41c2-ba04-d5bfb8057542",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = next(iter(embeddings.values())).shape[-1]\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "index.add(np.array(list(embeddings.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba6abea0-a74e-43ff-be5c-b4fc9f3f4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"fragment_index.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2ca6c3f-f317-4070-8b90-5a8a1e4d7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./embeddings_order.json\", \"w\") as f:\n",
    "    f.write(json.dumps(list(embeddings.keys())))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d734dc5-17b8-41c0-adf9-cf13e91de458",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_order = list(embeddings.keys())\n",
    "\n",
    "def search(query: str, k: int):\n",
    "    query_embed = embedding_model.encode([query], device=device)\n",
    "    distances, ids = index.search(query_embed, k=k)\n",
    "    chunk_ids = [\n",
    "        embeddings_order[idx] for idx in ids[0]\n",
    "    ]\n",
    "    return [\n",
    "        (distance, chunk_id, chunk_text_by_id[chunk_id])\n",
    "        for distance, chunk_id in zip(distances[0], chunk_ids)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35bac253-9db4-40b7-975c-c658d24cc750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(82.876724,\n",
       "  'transformers/docs/source/en/model_doc/phi3.md:0',\n",
       "  '<!--Copyright 2024 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Phi-3\\n\\n## Overview\\n\\nThe Phi-3 model was proposed in [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) by Microsoft.\\n\\n### Summary\\n\\nThe abstract from the Phi-3 paper is the following:\\n\\nWe introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).\\n\\nThe original code for Phi-3 can be found [here](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct).\\n\\n## Usage tips\\n\\n- This model is very similar to `Llama` with the main difference of [`Phi3SuScaledRotaryEmbedding`] and [`Phi3YarnScaledRotaryEmbedding`], where they are used to extend the context of the rotary embeddings. The query, key and values are fused, and the MLP\\'s up and gate projection layers are also fused.\\n- The tokenizer used for this model is identical to the [`LlamaTokenizer`], with the exception of additional tokens.\\n\\n## How to use Phi-3\\n\\n<Tip warning={true}>\\n\\nPhi-3 has been integrated in the development version (4.40.0.dev) of `transformers`. Until the official version is released through `pip`, ensure that you are doing one of the following:\\n\\n* When loading the model, ensure that `trust_remote_code=True` is passed as an argument of the `from_pretrained()` function.\\n\\n* Update your local `transformers` to the development version: `pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers`. The previous command is an alternative to cloning and installing from the source.\\n\\n</Tip>\\n\\n```python\\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n>>> model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\\n\\n>>> messages = [{\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}]\\n>>> inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\\n\\n>>> outputs = model.generate(inputs, max_new_tokens=32)\\n>>> text = tokenizer.batch_decode(outputs)[0]\\n>>> print(text)\\n<s><|user|> \\nCan you provide ways to eat combinations of bananas and dragonfruits?<|end|> \\n<|assistant|> \\nCertainly! Bananas and dragonfruits can be combined in various delicious ways. Here are some ideas for eating combinations of bananas and\\n```\\n\\n## Phi3Config\\n\\n[[autodoc]] Phi3Config\\n\\n<frameworkcontent>\\n<pt>\\n\\n## Phi3Model\\n\\n[[autodoc]] Phi3Model\\n    - forward\\n\\n## Phi3ForCausalLM\\n\\n[[autodoc]] Phi3ForCausalLM\\n    - forward\\n    - generate\\n\\n## Phi3ForSequenceClassification\\n\\n[[autodoc]] Phi3ForSequenceClassification\\n    - forward\\n\\n## Phi3ForTokenClassification\\n\\n[[autodoc]] Phi3ForTokenClassification\\n    - forward\\n\\n</pt>\\n</frameworkcontent>\\n'),\n",
       " (91.284004,\n",
       "  'transformers/docs/source/en/quantization/compressed_tensors.md:6',\n",
       "  '## Deep dive into a compressed-tensors model checkpoint\\n\\nIn this example we will examine how the compressed-tensors model nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf is defined through its configuration entry and see how this translates to the loaded model representation. \\n\\nFirst, let us look at the [`quantization_config` of the model](https://huggingface.co/nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf/blob/main/config.json). At a glance it looks overwhelming with the number of entries but this is because compressed-tensors is a format that allows for flexible expression both during and after model compression.\\n\\nIn practice for checkpoint loading and inference the configuration can be simplified to not include all the default or empty entries, so we will do that here to focus on what compression is actually represented.\\n\\n```yaml\\n\"quantization_config\": {\\n  \"config_groups\": {\\n    \"group_0\": {\\n      \"input_activations\": {\\n        \"num_bits\": 8,\\n        \"strategy\": \"tensor\",\\n        \"type\": \"float\"\\n      },\\n      \"targets\": [\"Linear\"],\\n      \"weights\": {\\n        \"num_bits\": 8,\\n        \"strategy\": \"tensor\",\\n        \"type\": \"float\"\\n      }\\n    }\\n  },\\n  \"format\": \"naive-quantized\",\\n  \"ignore\": [\"lm_head\"],\\n  \"quant_method\": \"compressed-tensors\",\\n  \"quantization_status\": \"frozen\"\\n},\\n```\\n\\nWe can see from the above configuration that it is specifying one config group that includes weight and activation quantization to FP8 with a static per-tensor strategy. It is also worth noting that in the `ignore` list there is an entry to skip quantization of the `lm_head` module, so that module should be untouched in the checkpoint.\\n\\nTo see the result of the configuration in practice, we can simply use the [safetensors viewer](https://huggingface.co/nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf?show_file_info=model.safetensors.index.json) on the model card to see the quantized weights, input_scale, and weight_scale for all of the Linear modules in the first model layer (and so on for the rest of the layers).\\n\\n| Tensors | Shape |\\tPrecision |\\n| ------- | ----- | --------- |\\nmodel.layers.0.input_layernorm.weight\\t| [4\\u202f096]\\t| BF16 \\nmodel.layers.0.mlp.down_proj.input_scale\\t| [1]\\t| BF16 \\nmodel.layers.0.mlp.down_proj.weight\\t| [4\\u202f096, 14\\u202f336] |\\tF8_E4M3 \\nmodel.layers.0.mlp.down_proj.weight_scale |\\t[1]\\t| BF16 \\nmodel.layers.0.mlp.gate_proj.input_scale |\\t[1]\\t| BF16 \\nmodel.layers.0.mlp.gate_proj.weight\\t| [14\\u202f336, 4\\u202f096]\\t| F8_E4M3 \\nmodel.layers.0.mlp.gate_proj.weight_scale\\t| [1] |\\tBF16 \\nmodel.layers.0.mlp.up_proj.input_scale|\\t[1]\\t|BF16 \\nmodel.layers.0.mlp.up_proj.weight |\\t[14\\u202f336, 4\\u202f096]\\t| F8_E4M3 \\nmodel.layers.0.mlp.up_proj.weight_scale | [1]\\t| BF16 \\nmodel.layers.0.post_attention_layernorm.weight |\\t[4\\u202f096]\\t|BF16 \\nmodel.layers.0.self_attn.k_proj.input_scale |\\t[1]\\t|  BF16\\nmodel.layers.0.self_attn.k_proj.weight |\\t[1\\u202f024, 4\\u202f096]|\\tF8_E4M3\\nmodel.layers.0.self_attn.k_proj.weight_scale |[1]\\t| BF16 \\nmodel.layers.0.self_attn.o_proj.input_scale\\t| [1]\\t| BF16\\nmodel.layers.0.self_attn.o_proj.weight | [4\\u202f096, 4\\u202f096]\\t| F8_E4M3 \\nmodel.layers.0.self_attn.o_proj.weight_scale | [1]\\t| BF16 \\nmodel.layers.0.self_attn.q_proj.input_scale\\t| [1]\\t| BF16 \\nmodel.layers.0.self_attn.q_proj.weight | [4\\u202f096, 4\\u202f096]\\t| F8_E4M3 \\nmodel.layers.0.self_attn.q_proj.weight_scale |\\t[1] | BF16 \\nmodel.layers.0.self_attn.v_proj.input_scale\\t| [1] | BF16 \\nmodel.layers.0.self_attn.v_proj.weight |\\t[1\\u202f024, 4\\u202f096]\\t| F8_E4M3 \\nmodel.layers.0.self_attn.v_proj.weight_scale |\\t[1] |\\tBF16 \\n\\nWhen we load the model with the compressed-tensors HFQuantizer integration, we can see that all of the Linear modules that are specified within the quantization configuration have been replaced by `CompressedLinear` modules that manage the compressed weights and forward pass for inference. Note that the `lm_head` mentioned before in the ignore list is still kept as an unquantized Linear module.\\n\\n```python\\nfrom transformers import AutoModelForCausalLM'),\n",
       " (96.34711,\n",
       "  'transformers/docs/source/en/model_doc/phi.md:2',\n",
       "  '## Overview\\n\\nThe Phi-1 model was proposed in [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Csar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sbastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li.\\n\\nThe Phi-1.5 model was proposed in [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, Sbastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.\\n\\n### Summary\\n\\nIn Phi-1 and Phi-1.5 papers, the authors showed how important the quality of the data is in training relative to the model size.\\nThey selected high quality \"textbook\" data alongside with synthetically generated data for training their small sized Transformer\\nbased model Phi-1 with 1.3B parameters. Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP.\\nThey follow the same strategy for Phi-1.5 and created another 1.3B parameter model with performance on natural language tasks comparable\\nto models 5x larger, and surpassing most non-frontier LLMs. Phi-1.5 exhibits many of the traits of much larger LLMs such as the ability\\nto think step by step or perform some rudimentary in-context learning.\\nWith these two experiments the authors successfully showed the huge impact of quality of training data when training machine learning models.\\n\\nThe abstract from the Phi-1 paper is the following:\\n\\n*We introduce phi-1, a new large language model for code, with significantly smaller size than\\ncompeting models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on\\n8 A100s, using a selection of textbook quality data from the web (6B tokens) and synthetically\\ngenerated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent\\nproperties compared to phi-1-base, our model before our finetuning stage on a dataset of coding\\nexercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as\\nphi-1 that still achieves 45% on HumanEval.*\\n\\nThe abstract from the Phi-1.5 paper is the following:\\n\\n*We continue the investigation into the power of smaller Transformer-based language models as\\ninitiated by TinyStories  a 10 million parameter model that can produce coherent English  and\\nthe follow-up work on phi-1, a 1.3 billion parameter model with Python coding performance close\\nto the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to\\ngenerate textbook quality data as a way to enhance the learning process compared to traditional\\nweb data. We follow the Textbooks Are All You Need approach, focusing this time on common\\nsense reasoning in natural language, and create a new 1.3 billion parameter model named phi-1.5,\\nwith performance on natural language tasks comparable to models 5x larger, and surpassing most\\nnon-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic\\ncoding. More generally, phi-1.5 exhibits many of the traits of much larger LLMs, both good such\\nas the ability to think step by step or perform some rudimentary in-context learning and bad,\\nincluding hallucinations and the potential for toxic and biased generations encouragingly though, we\\nare seeing improvement on that front thanks to the absence of web data. We open-source phi-1.5 to\\npromote further research on these urgent topics.*\\n\\nThis model was contributed by [Susnato Dhar](https://huggingface.co/susnato).\\n\\nThe original code for Phi-1, Phi-1.5 and Phi-2 can be found [here](https://huggingface.co/microsoft/phi-1), [here](https://huggingface.co/microsoft/phi-1_5) and [here](https://huggingface.co/microsoft/phi-2), respectively.\\n')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"How many layers are in a Phi 3 model?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a394a06e-4bc4-4656-aa14-ecb83cbb2910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading checkpoint shards:   0%|                                                                                                   | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|                                             | 1/2 [00:01<00:01,  1.14s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:01<00:00,  1.08it/s]\u001b[A\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_id),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65125343-cf8c-4d6a-bf1a-e80debd867dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_query(\n",
    "        query: str,\n",
    "        context: list[str],\n",
    ") -> str:\n",
    "    output = pipe([{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\\n\".join([\n",
    "            \" \".join([\n",
    "                \"You are a Huggingface Transformers expert.\",\n",
    "                \"Your responses may ONLY draw from the following context:\",\n",
    "            ]),\n",
    "            \"---- BEGIN CONTEXT ----\",\n",
    "            *context,\n",
    "            \"---- END CONTEXT ----\",\n",
    "        ])\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query,\n",
    "    }], max_new_tokens=1000)\n",
    "    return output[0]['generated_text'][-1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16ed07e4-87b3-4982-8fb5-ebdbbf8392b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_knowledge_assistant(query: str):\n",
    "    results = search(query, k=3)\n",
    "    response = respond_to_query(\n",
    "        query=query,\n",
    "        context=[\n",
    "            item[2]\n",
    "            for item in results\n",
    "        ]\n",
    "    )\n",
    "    print(response)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1475453-7e56-40c9-a6fe-fd3b5d6486f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The initial embedding layer in a transformer, often referred to as the token embedding layer, serves a crucial role in the transformer architecture. Its primary function is to convert the input data, which is typically a sequence of discrete tokens (like words in a text or patches in an image), into a continuous vector representation. This process is essential because transformers, like other deep learning models, operate on numerical data.\n",
      "\n",
      "\n",
      "In the context of natural language processing (NLP), the input tokens are usually words or subwords, and the token embedding layer transforms these discrete tokens into dense vectors of real numbers. These vectors capture the semantic and syntactic properties of the tokens, enabling the transformer to understand and process the input data effectively.\n",
      "\n",
      "\n",
      "For image processing tasks, as in the case of ViT (Vision Transformer), the input is a sequence of image patches. Each patch is first converted into a vector using a convolutional layer, which serves a similar purpose to token embeddings in NLP. This process ensures that the model can work with the input data in a form that is suitable for the subsequent self-attention mechanisms.\n",
      "\n",
      "\n",
      "The embedding layer is followed by a position embedding layer, which adds information about the position of each token in the sequence. This is crucial because transformers do not have an inherent sense of order or position in the sequence, unlike RNNs or CNNs. The position embeddings help the model understand the order of the tokens, which is essential for tasks like language modeling or image classification.\n",
      "\n",
      "\n",
      "In summary, the initial embedding layer in a transformer is responsible for converting the input data into a suitable format for the self-attention mechanisms to process, by creating dense vector representations for each token and adding positional information.\n"
     ]
    }
   ],
   "source": [
    "query_knowledge_assistant(\"What is the function of the initial embedding layer in a transformer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "896d9c5a-562b-4a2c-a43a-7cb5ca415b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To implement a novel attention mechanism in the Transformers library, you would need to follow a series of steps that involve understanding the current attention mechanisms, designing your novel approach, and then integrating it into the Transformers framework. Here's a high-level overview of the process:\n",
      "\n",
      "1. **Understand Existing Attention Mechanisms**: Familiarize yourself with the attention mechanisms currently available in the Transformers library, such as the original Transformer's self-attention, Scaled Dot-Product Attention (SDPA), and FlashAttention. Study their implementations, strengths, and limitations.\n",
      "\n",
      "2. **Design Your Novel Attention Mechanism**: Based on your understanding, conceptualize a novel attention mechanism that addresses specific limitations or introduces new capabilities. Consider factors like computational efficiency, memory usage, and adaptability to different tasks.\n",
      "\n",
      "3. **Implement the Novel Attention Mechanism**:\n",
      "   - Write the core attention computation code in Python, ensuring it's efficient and well-documented.\n",
      "   - Test your implementation with synthetic data to verify its correctness.\n",
      "\n",
      "4. **Integrate with Transformers**:\n",
      "   - Create a new class that extends the `nn.Module` class from PyTorch.\n",
      "   - Implement the forward method to apply your attention mechanism to the input tensors.\n",
      "   - Ensure your class can be used interchangeably with existing attention mechanisms in Transformers.\n",
      "\n",
      "5. **Benchmarking**:\n",
      "   - Compare your attention mechanism's performance with existing ones on standard benchmarks.\n",
      "   - Optimize your implementation for speed and memory usage.\n",
      "\n",
      "6. **Contribute to the Transformers Library**:\n",
      "   - Fork the Transformers repository or create a new branch in the main repository.\n",
      "   - Write a `README.md` file explaining your attention mechanism, its advantages, and use cases.\n",
      "   - Create a `setup.py` file if your attention mechanism is a standalone package.\n",
      "   - Write unit tests for your attention mechanism to ensure its correctness and robustness.\n",
      "   - Submit a pull request to the Transformers repository with your attention mechanism.\n",
      "\n",
      "7. **Documentation and Community Feedback**:\n",
      "   - Document your attention mechanism thoroughly in the `README.md` and possibly in a separate `docs` directory.\n",
      "   - Engage with the community through forums, mailing lists, or social media to gather feedback and collaborate.\n",
      "\n",
      "Here's a simplified example of how you might start implementing a new attention mechanism:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class NovelAttention(nn.Module):\n",
      "    def __init__(self, dim, num_heads):\n",
      "        super(NovelAttention, self).__init__()\n",
      "        self.num_heads = num_heads\n",
      "        self.dim = dim\n",
      "        self.query = nn.Linear(dim, dim)\n",
      "        self.key = nn equivalents to the original Transformer's self-attention mechanism.\n",
      "\n",
      "    def forward(self, x):\n",
      "        batch_size = x.size(0)\n",
      "\n",
      "        # Linear projections\n",
      "        query = self.query(x).view(batch_size, -1, self.num_heads, self.dim // self.num_heads).transpose(1, 2)\n",
      "        key = self.key(x).view(batch_size, -1, self.num_heads, self.dim // self.num_heads).transpose(1, 2)\n",
      "\n",
      "        # Scaled dot-product attention\n",
      "        attn_weights = torch.matmul(query, key.transpose(-2, -1))\n",
      "        attn_weights = attn_weights / math.sqrt(self.dim // self.num_heads)\n",
      "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
      "\n",
      "        # Apply attention to value\n",
      "        value = self.value(x).view(batch_size, -1, self.num_heads, self.dim // self.num_heads).transpose(1, 2)\n",
      "        attention_output = torch.matmul(attn_weights, value)\n",
      "        attention_output = attention_output.transpose(2, 1).contiguous().view(\n"
     ]
    }
   ],
   "source": [
    "query_knowledge_assistant(\"How might I implement a novel attention mechanism in the transformers library?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "name": "hf_docs_rag.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
